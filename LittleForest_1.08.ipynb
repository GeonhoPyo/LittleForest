{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, r2_score\n",
    "from sklearn.linear_model import Lasso, LassoCV, MultiTaskLassoCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import optimizers\n",
    "from keras.initializers import lecun_normal\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission-----------\n",
      "['id', 'hhb', 'hbo2', 'ca', 'na']\n",
      "(10000, 5)\n",
      "test-----------\n",
      "['id', 'rho', '650_src', '660_src', '670_src', '680_src', '690_src', '700_src', '710_src', '720_src', '730_src', '740_src', '750_src', '760_src', '770_src', '780_src', '790_src', '800_src', '810_src', '820_src', '830_src', '840_src', '850_src', '860_src', '870_src', '880_src', '890_src', '900_src', '910_src', '920_src', '930_src', '940_src', '950_src', '960_src', '970_src', '980_src', '990_src', '650_dst', '660_dst', '670_dst', '680_dst', '690_dst', '700_dst', '710_dst', '720_dst', '730_dst', '740_dst', '750_dst', '760_dst', '770_dst', '780_dst', '790_dst', '800_dst', '810_dst', '820_dst', '830_dst', '840_dst', '850_dst', '860_dst', '870_dst', '880_dst', '890_dst', '900_dst', '910_dst', '920_dst', '930_dst', '940_dst', '950_dst', '960_dst', '970_dst', '980_dst', '990_dst']\n",
      "(10000, 72)\n",
      "train-----------\n",
      "['id', 'rho', '650_src', '660_src', '670_src', '680_src', '690_src', '700_src', '710_src', '720_src', '730_src', '740_src', '750_src', '760_src', '770_src', '780_src', '790_src', '800_src', '810_src', '820_src', '830_src', '840_src', '850_src', '860_src', '870_src', '880_src', '890_src', '900_src', '910_src', '920_src', '930_src', '940_src', '950_src', '960_src', '970_src', '980_src', '990_src', '650_dst', '660_dst', '670_dst', '680_dst', '690_dst', '700_dst', '710_dst', '720_dst', '730_dst', '740_dst', '750_dst', '760_dst', '770_dst', '780_dst', '790_dst', '800_dst', '810_dst', '820_dst', '830_dst', '840_dst', '850_dst', '860_dst', '870_dst', '880_dst', '890_dst', '900_dst', '910_dst', '920_dst', '930_dst', '940_dst', '950_dst', '960_dst', '970_dst', '980_dst', '990_dst', 'hhb', 'hbo2', 'ca', 'na']\n",
      "(10000, 76)\n"
     ]
    }
   ],
   "source": [
    "# CSV TO DATA\n",
    "def csvToData(csv_file):\n",
    "    df = pd.read_csv(csv_file, header = 0)\n",
    "    column = list(df.columns.values)\n",
    "    data = df.values\n",
    "    return column, data\n",
    "\n",
    "sample_column, sample_csv_data = csvToData(\"data/sample_submission.csv\")\n",
    "test_column, test_csv_data = csvToData(\"data/test.csv\")\n",
    "train_column, train_csv_data = csvToData(\"data/train.csv\")\n",
    "print('sample_submission-----------')\n",
    "print(sample_column)\n",
    "print(sample_csv_data.shape)\n",
    "print('test-----------')\n",
    "print(test_column)\n",
    "print(test_csv_data.shape)\n",
    "print('train-----------')\n",
    "print(train_column)\n",
    "print(train_csv_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5) (10000, 72) (10000, 76)\n"
     ]
    }
   ],
   "source": [
    "# Data to DataFrame\n",
    "sample_data = pd.DataFrame(sample_csv_data, columns=sample_column)\n",
    "test_data = pd.DataFrame(test_csv_data, columns=test_column)\n",
    "train_data = pd.DataFrame(train_csv_data, columns=train_column)\n",
    "print(sample_data.shape, test_data.shape, train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list=['650_src', '660_src', '670_src', '680_src', '690_src', '700_src', '710_src', '720_src', '730_src', \n",
    "          '740_src', '750_src', '760_src', '770_src', '780_src', '790_src', '800_src', '810_src', '820_src', \n",
    "          '830_src', '840_src', '850_src', '860_src', '870_src', '880_src', '890_src', '900_src', '910_src', \n",
    "          '920_src', '930_src', '940_src', '950_src', '960_src', '970_src', '980_src', '990_src']\n",
    "dst_list=['650_dst', '660_dst', '670_dst', '680_dst', '690_dst', '700_dst', '710_dst', '720_dst', '730_dst', \n",
    "          '740_dst', '750_dst', '760_dst', '770_dst', '780_dst', '790_dst', '800_dst', '810_dst', '820_dst', \n",
    "          '830_dst', '840_dst', '850_dst', '860_dst', '870_dst', '880_dst', '890_dst', '900_dst', '910_dst', \n",
    "          '920_dst', '930_dst', '940_dst', '950_dst', '960_dst', '970_dst', '980_dst', '990_dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:04<00:00, 2211.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:04<00:00, 2201.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 보간법 tqdm , \n",
    "from tqdm import tqdm\n",
    "alpha=train_data[dst_list]\n",
    "beta=test_data[dst_list]\n",
    "\n",
    "for i in tqdm(train_data.index):\n",
    "    alpha.loc[i] = alpha.loc[i].interpolate()\n",
    "    \n",
    "for i in tqdm(test_data.index):\n",
    "    beta.loc[i] = beta.loc[i].interpolate()\n",
    "    \n",
    "#위에 보간법을 사용하게되면, 앞단에 비는 경우가 발생. 빌 경우 뒷단에 있는 데이터를 끌어다가 씀.\n",
    "alpha.loc[alpha['700_dst'].isnull(),'700_dst']=alpha.loc[alpha['700_dst'].isnull(),'710_dst']\n",
    "alpha.loc[alpha['690_dst'].isnull(),'690_dst']=alpha.loc[alpha['690_dst'].isnull(),'700_dst']\n",
    "alpha.loc[alpha['680_dst'].isnull(),'680_dst']=alpha.loc[alpha['680_dst'].isnull(),'690_dst']\n",
    "alpha.loc[alpha['670_dst'].isnull(),'670_dst']=alpha.loc[alpha['670_dst'].isnull(),'680_dst']\n",
    "alpha.loc[alpha['660_dst'].isnull(),'660_dst']=alpha.loc[alpha['660_dst'].isnull(),'670_dst']\n",
    "alpha.loc[alpha['650_dst'].isnull(),'650_dst']=alpha.loc[alpha['650_dst'].isnull(),'660_dst']\n",
    "\n",
    "beta.loc[beta['700_dst'].isnull(),'700_dst']=beta.loc[beta['700_dst'].isnull(),'710_dst']\n",
    "beta.loc[beta['690_dst'].isnull(),'690_dst']=beta.loc[beta['690_dst'].isnull(),'700_dst']\n",
    "beta.loc[beta['680_dst'].isnull(),'680_dst']=beta.loc[beta['680_dst'].isnull(),'690_dst']\n",
    "beta.loc[beta['670_dst'].isnull(),'670_dst']=beta.loc[beta['670_dst'].isnull(),'680_dst']\n",
    "beta.loc[beta['660_dst'].isnull(),'660_dst']=beta.loc[beta['660_dst'].isnull(),'670_dst']\n",
    "beta.loc[beta['650_dst'].isnull(),'650_dst']=beta.loc[beta['650_dst'].isnull(),'660_dst']\n",
    "\n",
    "train_data[dst_list] = np.array(alpha)\n",
    "test_data[dst_list] = np.array(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho 값의 제곱을 dst에 곱해줌.\n",
    "for col in dst_list:\n",
    "    train_data[col] = train_data[col] * (train_data['rho'] ** 2)\n",
    "    test_data[col] = test_data[col] * (test_data['rho']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원래 빛과 측정빛의 차이값을 변수로 추가\n",
    "gap_feature_names=[]\n",
    "for i in range(650, 1000, 10):\n",
    "    gap_feature_names.append(str(i) + '_gap')\n",
    "\n",
    "alpha=pd.DataFrame(np.array(train_data[src_list]) - np.array(train_data[dst_list]), columns=gap_feature_names, index=train_data.index)\n",
    "beta=pd.DataFrame(np.array(test_data[src_list]) - np.array(test_data[dst_list]), columns=gap_feature_names, index=test_data.index)\n",
    "\n",
    "train_data=pd.concat((train_data, alpha), axis=1)\n",
    "test_data=pd.concat((test_data, beta), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원래 빛과 측정빛의 차이 비율을 변수 추가\n",
    "epsilon=1e-10\n",
    "\n",
    "for dst_col, src_col in zip(dst_list, src_list):\n",
    "    dst_val=train_data[dst_col]\n",
    "    src_val=train_data[src_col] + epsilon\n",
    "    \n",
    "    delta_ratio = dst_val / src_val\n",
    "    train_data[dst_col + '_' + src_col + '_ratio'] = delta_ratio\n",
    "    \n",
    "    dst_val=test_data[dst_col]\n",
    "    src_val=test_data[src_col] + epsilon\n",
    "    \n",
    "    delta_ratio = dst_val / src_val\n",
    "    test_data[dst_col + '_' + src_col + '_ratio'] = delta_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:13<00:00, 729.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:13<00:00, 750.36it/s]\n"
     ]
    }
   ],
   "source": [
    "##측정 빛에 이산 푸리에 변환\n",
    "alpha_real=train_data[dst_list]\n",
    "alpha_imag=train_data[dst_list]\n",
    "\n",
    "beta_real=test_data[dst_list]\n",
    "beta_imag=test_data[dst_list]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "for i in tqdm(beta_real.index):\n",
    "    beta_real.loc[i]=beta_real.loc[i] - beta_real.loc[i].mean()\n",
    "    beta_imag.loc[i]=beta_imag.loc[i] - beta_imag.loc[i].mean()\n",
    "    \n",
    "    beta_real.loc[i] = np.fft.fft(beta_real.loc[i], norm='ortho').real\n",
    "    beta_imag.loc[i] = np.fft.fft(beta_imag.loc[i], norm='ortho').imag\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in dst_list:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "beta_real.columns=real_part\n",
    "beta_imag.columns=imag_part\n",
    "beta=pd.concat((beta_real, beta_imag), axis=1)\n",
    "\n",
    "train_data=pd.concat((train_data, alpha), axis=1)\n",
    "test_data=pd.concat((test_data, beta), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(train_data).values.any(),np.isnan(test_data).values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "training data ( 8000개 ) = train_dataset\n",
    "\n",
    "validataion data ( 2000개 ) = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 215) (2000, 215)\n"
     ]
    }
   ],
   "source": [
    "num_traindata = 8000\n",
    "num_validdata = 2000\n",
    "from sklearn.utils import shuffle\n",
    "train_data = shuffle(train_data)\n",
    "train_dataset = train_data[:num_traindata]\n",
    "valid_dataset = train_data[num_traindata : num_traindata + num_validdata]\n",
    "train_dataset = train_dataset.drop('id',axis = 1)\n",
    "valid_dataset = valid_dataset.drop('id',axis = 1)\n",
    "print(train_dataset.shape, valid_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "--training data ( 8000 )\n",
    "\n",
    "x_train\n",
    "\n",
    "y_train\n",
    "\n",
    "--validation data ( 2000 )\n",
    "\n",
    "x_test\n",
    "\n",
    "y_test\n",
    "\n",
    "--training data ( 10000 )\n",
    "\n",
    "x_train_sample \n",
    "\n",
    "y_train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 211), (8000, 4))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train, y_train 분류\n",
    "x_train = train_dataset.loc[:, 'rho':] # train dataset\n",
    "x_train.drop(['hhb','hbo2','ca','na'], axis='columns', inplace=True)\n",
    "y_train = train_dataset.loc[:, 'hhb':'na'] # train labels\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 211), (2000, 4))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_test, y_test 분류\n",
    "x_test = valid_dataset.loc[:, 'rho':] # test dataset\n",
    "x_test.drop(['hhb','hbo2','ca','na'], axis='columns', inplace=True)\n",
    "y_test = valid_dataset.loc[:, 'hhb':'na'] # test labels\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 211), (10000, 4))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample Code 용\n",
    "x_train_sample = train_data.loc[:, 'rho':] # train dataset\n",
    "x_train_sample.drop(['hhb','hbo2','ca','na'], axis='columns', inplace=True)\n",
    "y_train_sample = train_data.loc[:, 'hhb':'na'] # train labels\n",
    "x_train_sample.shape, y_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhb\n",
      "[0]\ttrain-mae:5.29064\teval-mae:5.27029\n",
      "[999]\ttrain-mae:0.000595\teval-mae:0.866566\n",
      "[0]\ttrain-mae:5.30938\teval-mae:5.19686\n",
      "[999]\ttrain-mae:0.000609\teval-mae:0.829306\n",
      "[0]\ttrain-mae:5.29878\teval-mae:5.23914\n",
      "[999]\ttrain-mae:0.000602\teval-mae:0.866656\n",
      "[0]\ttrain-mae:5.27926\teval-mae:5.3544\n",
      "[999]\ttrain-mae:0.00063\teval-mae:0.879947\n",
      "[0]\ttrain-mae:5.26385\teval-mae:5.4172\n",
      "[999]\ttrain-mae:0.000625\teval-mae:0.887416\n",
      "hbo2\n",
      "[0]\ttrain-mae:2.45828\teval-mae:2.49552\n",
      "[999]\ttrain-mae:0.00064\teval-mae:0.596674\n",
      "[0]\ttrain-mae:2.47092\teval-mae:2.42735\n",
      "[999]\ttrain-mae:0.000637\teval-mae:0.584332\n",
      "[0]\ttrain-mae:2.46294\teval-mae:2.47591\n",
      "[999]\ttrain-mae:0.00063\teval-mae:0.590059\n",
      "[0]\ttrain-mae:2.45718\teval-mae:2.49673\n",
      "[999]\ttrain-mae:0.000614\teval-mae:0.586745\n",
      "[0]\ttrain-mae:2.46664\teval-mae:2.43355\n",
      "[999]\ttrain-mae:0.000616\teval-mae:0.585503\n",
      "ca\n",
      "[0]\ttrain-mae:5.99222\teval-mae:6.08401\n",
      "[999]\ttrain-mae:0.000642\teval-mae:1.86966\n",
      "[0]\ttrain-mae:6.00984\teval-mae:5.98616\n",
      "[999]\ttrain-mae:0.000617\teval-mae:1.91088\n",
      "[0]\ttrain-mae:5.99958\teval-mae:6.01989\n",
      "[999]\ttrain-mae:0.000651\teval-mae:1.9367\n",
      "[0]\ttrain-mae:6.01001\teval-mae:5.94358\n",
      "[999]\ttrain-mae:0.000638\teval-mae:1.94708\n",
      "[0]\ttrain-mae:6.00455\teval-mae:6.00013\n",
      "[999]\ttrain-mae:0.000606\teval-mae:1.93629\n",
      "na\n",
      "[0]\ttrain-mae:2.04286\teval-mae:2.0659\n",
      "[999]\ttrain-mae:0.000617\teval-mae:1.30453\n",
      "[0]\ttrain-mae:2.04619\teval-mae:2.0804\n",
      "[999]\ttrain-mae:0.000663\teval-mae:1.26266\n",
      "[0]\ttrain-mae:2.04124\teval-mae:2.0818\n",
      "[999]\ttrain-mae:0.000632\teval-mae:1.29248\n",
      "[0]\ttrain-mae:2.0531\teval-mae:2.05061\n",
      "[999]\ttrain-mae:0.000596\teval-mae:1.28088\n",
      "[0]\ttrain-mae:2.05413\teval-mae:2.04522\n",
      "[999]\ttrain-mae:0.000627\teval-mae:1.33558\n"
     ]
    }
   ],
   "source": [
    "#Sample_code\n",
    "avr_valid_mse = 0\n",
    "def test_train_model(x_data, y_data, k=5):\n",
    "    models = []\n",
    "    \n",
    "    k_fold = KFold(n_splits=k, shuffle=True, random_state=123)\n",
    "    \n",
    "    for train_idx, val_idx in k_fold.split(x_data):\n",
    "        \n",
    "        x_train, y_train = x_data.iloc[train_idx], y_data.iloc[train_idx]\n",
    "        x_val, y_val = x_data.iloc[val_idx], y_data.iloc[val_idx]\n",
    "    \n",
    "        d_train = xgb.DMatrix(data = x_train, label = y_train)\n",
    "        d_val = xgb.DMatrix(data = x_val, label = y_val)\n",
    "        \n",
    "        wlist = [(d_train, 'train'), (d_val, 'eval')]\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'mae',\n",
    "            'seed':777\n",
    "            }\n",
    "\n",
    "\n",
    "        model = xgb.train(params=params, dtrain=d_train, num_boost_round=1000, verbose_eval=1000, evals=wlist)\n",
    "        models.append(model)\n",
    "        \n",
    "        \n",
    "\n",
    "    return models\n",
    "\n",
    "models = {}\n",
    "for label in y_train.columns:\n",
    "    print(label)\n",
    "    models[label] = test_train_model(x_train_sample, y_train_sample[label])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhb\n",
      "hbo2\n",
      "ca\n",
      "na\n"
     ]
    }
   ],
   "source": [
    "#결과 출력\n",
    "for col in models:\n",
    "    print(col)\n",
    "    preds = []\n",
    "    for model in models[col]:\n",
    "        preds.append(model.predict(xgb.DMatrix(test_data.loc[:, 'rho':])))\n",
    "    pred = np.mean(preds, axis=0)\n",
    "\n",
    "    sample_data[col] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hhb</th>\n",
       "      <th>hbo2</th>\n",
       "      <th>ca</th>\n",
       "      <th>na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>9.018572</td>\n",
       "      <td>4.452666</td>\n",
       "      <td>8.005127</td>\n",
       "      <td>2.846764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>7.164327</td>\n",
       "      <td>2.323943</td>\n",
       "      <td>9.818594</td>\n",
       "      <td>2.752697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>9.284598</td>\n",
       "      <td>4.968606</td>\n",
       "      <td>11.628729</td>\n",
       "      <td>3.780931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>8.377440</td>\n",
       "      <td>4.102247</td>\n",
       "      <td>9.838860</td>\n",
       "      <td>4.921188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>7.568151</td>\n",
       "      <td>3.207137</td>\n",
       "      <td>8.396446</td>\n",
       "      <td>4.091807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       hhb      hbo2         ca        na\n",
       "0  10000  9.018572  4.452666   8.005127  2.846764\n",
       "1  10001  7.164327  2.323943   9.818594  2.752697\n",
       "2  10002  9.284598  4.968606  11.628729  3.780931\n",
       "3  10003  8.377440  4.102247   9.838860  4.921188\n",
       "4  10004  7.568151  3.207137   8.396446  4.091807"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Excel \n",
    "sample_data.to_csv('Dacon_Geonho_20200616_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
